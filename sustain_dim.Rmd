---
title: "Identifying sustainability dimensions of corporate reports by directed topic analysis"
author: "Maria Osipenko"
date: '2023-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Directed topic extraction with side information. 

Topic analysis represents each document of a text corpus in a low dimensional latent topic space. In some cases the desired topic representation is prestructured in form of requirements or guidelines delivering side information. For instance, investors can be interested in automatically assessing sustainability in textual content of corporate reports with a focus on the established 17 UN sustainability goals. The main corpus here contains the corporate report texts, and the texts with the definitions of the 17 UN sustainability goals represent the side information. Under assumption that both text corpora share a common low dimensional subspace, we propose to represent them in a such via directed topic extraction by matrix co-factorization. Both, the main and the side text corpora are first represented as term-document matrices, which are then jointly decomposed into word-topic and topic-document matrices. Thereby, the word-topic matrix is common to both text corpora, whereas the topic-document matrices contain specific representations in the shared topic space. A nuisance parameter, which allows to move focus between error minimization of individual factorization terms, controls the extent, to which the side information is taken into account. With our approach, documents from the main and the side corpora can be related to each other in the resulting latent topic space. That is, the considered corporate reports are represented in the same latent topic space as the descriptions of the 17 UN sustainability goals, such that a structured automatic sustainability assessment of textual reports content is possible. We provide an algorithm for such directed topic extraction and propose techniques for visualizing and interpreting the results.

## Idea

The market for sustainable investments grows steadily. However, there are no uniform standards for comparing/quantifying sustainability levels of firms. Although several agencies provide in the mean time environmental, social, governance (ESG) rating, @berg2022 points out the disagreement of such ratings across the rating agencies. In this situation, it seems hard to overview the ESG developement of potential investment firms and decide upon investors ESG value system.

We build upon an independent well defined ESG value system - the 17 sustainable development goals (SDG) of the United Nations (https://sdgs.un.org/goals).
Quantifying sustainability with respect to SGDs:

- do firms products contribute to sustainable development?
- SDG - intergovernmental set of 17 goals which broadly address modern environmental and social challenges adopted in 2015 by the UN General Assembly 
- identify companies that contribute positively to solving those major challenges.


-	Topic analysis represents each document in a collection of documents in a low dimensional latent topic space

- Supervised and unsupervised approaches are possible. 

- Due to limitations of supervised techniques (costs for labeling, subjectivity) we use an unsupervised methodology for embedding in a low dimensional space.

- However, sometimes the latent space is prestructured by e.g. policy maker as some sort of regulation requirement, on which the documents are matched. 

- We model the later given structure by stratifying to match the occurrences of terms with a  specific keyword collection from the regulatory texts. In the case of sustainability reports - 17 sustainability goals.

- As a result, a low dimensional representation of phrases in the report sentences and the sentences themselves stratified by the defined sustainability goals is obtained.

- Use data from @kang2022

For a comprehensive review of text mining in finance literature, we refer to @gupta2020.

@Harandizadeh2022 proposes to use word2vec embeddings combined with LDA and vocabulary priors to obtain interpretable word embeddings.
@eshima2023 embed prespecified keywords in LDA for the same reason. In the same spirit, @watanabe2022 use seeded LDA with a carefuly chosen seeded vocabulary to assist in classifying documents in specific categories. With their approaches, the authors embedd additional information in topic extraction.

The draw back of the mentioned approaches in @watanabe2022 and @eshima2023 lies in the need of manual intervention for keyword or vocabulary specification. @Harandizadeh2022 uses word vectors from a pretrained general purpose word2vec model and thus, it is not clear, whether their model works for specific domains as sustainability reports.

@rao2015 and later @zahng2020_graph propose to integrate side information using graphs. They propose a graph regularized version of matrix factorization and an associated alternating algorithm. However, their side information is not high dimensional and incorporates few individual characteristics which build basis for the graph links.

Another way to consider such additional information are matrix co-factorization techniques, which are especially popular in recommended systems. Co-factorization techniques factorize two or three matrices with some common cofactors simultaneously. For instance, @fang2011 consider user communities information and @luo2019 incorporate tagging and time stamp of ratings in their personalized recommendations via matrix co-factorization. The approach is transparent and easily adjustable. By introducing a nuisance parameter which allows to move focus between error minimization of individual factorization terms, additional flexibility is ensured.

For this reasons, we follow the later approach and propose to use matrix co-factorization for embedding additional information from sustainable goals definition into topics extraction from corporate reports.

## Model

We assume that the corporate reports texts share a common topic structure with the sustainability goals definition but also contain some other topics concerning e.g. financial statements. Moreover, we anticipate that the goals are written very focused using concrete sparse vocabulary, whereas the reports may refer to the same concepts using other wordings. That is, a common topic may contain words that are semantically relevant to sustainability goals vocabulary but not directly mentioned in the texts of the SDGs.
That is, we assume, that both text corpora share a common low dimensional subspace, in which they can be compared to each other by means of some distance measure.

To account for the mentioned issues, we define the following model for terms-document matrices arising from reports  and sustainability goals texts.

\[M = U^\top V + E\]

and 

\[C = {U_{1:k}}^\top Q + F\]

where $U_{1:k}$ is a matrix which contains only the first $k$ rows of $U$ and

- $M$ is the term-document matrix for the corporate reports with dimensions $(p\times n)$, where $p$ is the joint vocabulary (words and phrases with two co-occurring words) obtained from both reports and sustainability goals texts. $n$ is the number of corporate reports contexts, where the later represents one page of a corporate report. The overall dimensions for $M$ for our data are $(18'086\times 6'891)$.
- $C$ is the term-document matrix for the sustainability goals with dimensions $(p\times m)$, where $p$ is again the joint vocabulary (words and phrases with two co-occurring words) obtained from both reports and sustainability goals texts. $m$ is the number of sustainability goals contexts, where each context represents each of the $17$ goals. The overall dimensions for $C$ for our data are $(18'086\times 17)$.
- $U$ is the word-topic representation matrix of dimensions $(p\times k+\tilde k)$, where $k$ is the number of common topics and $\tilde k$ is the number of report specific topics.
- $V$ is the context-topic representation matrix for the reports of dimensions $(k+\tilde k\times n)$.
- $Q$ is the context-topic representation matrix for sustainability goals of dimensions $(k\times m)$.
- $E$ and $F$ are matrices of error terms of dimensions $(p\times n)$ and $(p\times m)$ respectively.

The associated topic extraction problem is then:

\[\min(||M - U^\top V||^2 + \lambda ||C-U_0^\top Q||^2)\]

where $\lambda$ adapts the importance of the loss on the second factorization term.

Because of the non-negativity of the entries in $M$ and $C$ it makes sense to restrict at least $U$ to be non-negative. This enhances the interpretability and sparsity of the resulting topics **(see cite)**.

The value of $\lambda$ balances out the combined loss function. It is responsible for adjusting the impact of accuracy concerning reports versus SDGs. Since the second dimension of $C$ is much lower than that of $M$, the first part of the loss will dominate the co-factorization. To give more weight to the second part one can alternate $\lambda$.

In summary, with the resulting low dimensional representation of corporate reports together with SDGs, we create a basis for choosing, evaluating and monitoring investments with respect to their impact on society and the environment.



```{r}
rm(list=ls())
source("functions.R")
```


## Data 

https://github.com/llbtl/paper_ssm01/tree/main/data


```{r}
# #files<-list.files("data_pdf/reports",full.names = T)
# files<-list.files("data_pdf/reports_tech",full.names = T)
# 
# contents<-paragraphs<-data.frame()
# 
# for (f in 1:length(files)){
#     #print(files[f])
#     content<-Rpoppler::PDF_text(files[f])
#     contents<-rbind(contents, data.frame(text=content,firm=rep(files[f],length(content))))
#     paragraph<-unlist(strsplit(content, "\n[0-9]"))#too long
#     paragraph<-paragraph[nchar(paragraph)>300]
#     paragraphs<-rbind(paragraphs, data.frame(text=paragraph,firm=rep(files[f],length(paragraph))))
# }
# 
# preproc_content<-function(contents,thr=300){
#   contents<-contents[nchar(as.character(contents[,1]))>thr,]
#   contents[,1]<-gsub(pattern="\n[0-9]",replacement=" ",x=as.character(contents[,1]))
#   contents[,1]<-gsub(pattern="-\n",replacement="",x=as.character(contents[,1]))
#   contents[,1]<-gsub(pattern="\n",replacement=" ",x=as.character(contents[,1]))
#   return(contents)
# }
# 
# contents<-preproc_content(contents)
# paragraphs<-preproc_content(paragraphs)
# 
# 
# 

```


```{r}
# data<-contents
# data$text<-as.character(data$text)
# # save(data, file="data_sustain.RData")
# save(data, file="data_sustain_tech.RData")
# 
# # data<-paragraphs
# # data$text<-as.character(data$text)
# # save(data, file="data_sustain_paragr.RData")
```

```{r}
#load("data_sustain_paragr.RData")
load("data_sustain_tech.RData")
```


## Data

Use R-Package Quanteda (Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A (2018). “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software, 3(30), 774. doi: 10.21105/joss.00774, https://quanteda.io.) to set up a corpus, to split in tokens, and compute the relative frequencies.

corporate reports

```{r, warnings=F, message=FALSE}
library(quanteda)
data$id<-1:nrow(data)
mycorpus<-corpus(data, docid_field="id",text_field="text")
print(paste("Number of docs is",ndoc(mycorpus)))
#tokenize
tk<-tokenizers::tokenize_word_stems(data[,1], stopwords = c(stopwords::stopwords("en"),"toyota", "walmart","nestle","basf","ikea","ms","m&s",
                                                            "intel"#, "microsoft","google","apple","ibm","amazon"
                                                            ,2011:2022))
tk<-tokens(as.tokens(tk), remove_punct=TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens=T)
tk2<-tokens_ngrams(as.tokens(tk),n=1:2,skip=2)
#dfm
# mydfm<-dfm(tk2, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
mydfm<-dfm(tk, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()


words2remove<-c("[[:digit:]]","toyota", "walmart","nestle","basf","ikea","ms","m&s","per", "wal", "mart","nestlé","bayer","www.sdgcompass.org", "e.g","i’m","said",
                "ulrich", "han", "wayn", "heinz", "smith", "brudermül", "kurt", "suckal", "margret","harald", "gandhi", "sanjeev",  "michael", "andrea",
                "franz", "diekmann", "jürgen" , "hambrecht" ,   "schäferkordt","robert","e.u" ,"kpmg" )

#keep only terms with more than three chars
mydfm<-dfm_select(mydfm,pattern=words2remove, valuetype="regex",selection="remove",min_nchar = 3)
mydfm<-dfm_select(mydfm,pattern=stopwords::stopwords("en"),valuetype="fixed",selection="remove")
#keep only words with frequency >=10
mydfm<-dfm_trim(mydfm, min_docfreq = 0.005, max_docfreq=0.5,docfreq_type="prop")
#take relative frequencies as entries
#mydfm_w<-dfm_weight(mydfm, scheme = "prop")

```

sustainability goals (https://www.un.org/sustainabledevelopment/)

```{r}
# files<-list.files("data_pdf/SDGs",full.names = T,recursive = T)
# 
# contents<-data.frame()
# 
# for (f in 1:length(files)){
#     #print(files[f])
#     content<-Rpoppler::PDF_text(files[f])
#     contents<-rbind(contents, data.frame(text=content,firm=rep(files[f],length(content))))
# }
# contents<-contents[nchar(as.character(contents[,1]))>300,]
# contents[,1]<-gsub(pattern="\n[0-9]",replacement=" ",x=as.character(contents[,1]))
# contents[,1]<-gsub(pattern="-\n",replacement="",x=as.character(contents[,1]))
# contents[,1]<-gsub(pattern="\n",replacement=" ",x=as.character(contents[,1]))
```

```{r}
# goals<-contents
# #goals$text<-as.character(data$text)
# save(goals, file="goals_sustain.RData")
```

```{r}
load("goals_sustain.RData")
```


```{r, warnings=F, message=FALSE}
library(quanteda)
goals$id<-1:nrow(goals)
mycorpus<-corpus(goals, docid_field="id",text_field="text")
print(paste("Number of docs is",ndoc(mycorpus)))
#tokenize
tkk<-tokenizers::tokenize_word_stems(goals[,1], stopwords = c(stopwords::stopwords("en")))
tkk<-tokens(as.tokens(tkk), remove_punct=TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens=T)
tk22<-tokens_ngrams(as.tokens(tkk),n=1:2,skip=2)
#dfm
# mydfm2<-dfm(tk22, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
mydfm2<-dfm(tkk, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
#keep only terms with more than two chars
mydfm2<-dfm_select(mydfm2,pattern=words2remove, valuetype="regex",selection="remove",min_nchar = 3)
mydfm2<-dfm_select(mydfm2,pattern=stopwords::stopwords("en"),valuetype="fixed",selection="remove")
#keep only words with frequency >=2
mydfm2<-dfm_trim(mydfm2, min_docfreq = 0.01, max_docfreq=0.5,docfreq_type="prop")
#take relative frequencies as entries
#mydfm_w<-dfm_weight(mydfm, scheme = "prop")
#mydfm2

mydfm2<-dfm_group(mydfm2,goals$firm)
```


combine the features

```{r}
myfeatures<-c(featnames(mydfm), featnames(mydfm2))
myfeatures<-myfeatures[!duplicated(myfeatures)]
```


```{r}
#mydfm_n["text1","group"]/sum(mydfm_n["text1",])#0.01680672

```


```{r}
mydfm_n<-dfm_match(mydfm, features=myfeatures)
mydfmw_n<-dfm_tfidf(mydfm_n)
#mydfmw_n<-dfm_weight(mydfm_n,scheme="prop")
mydfm_n2<-dfm_match(mydfm2, features=myfeatures)
mydfm_n2
mydfmw_n2<-dfm_tfidf(mydfm_n2)
#mydfmw_n2<-dfm_weight(mydfm_n2,scheme="prop")
```

convert to matrix


```{r}
## export as matrix
dtm<-as.matrix(mydfm_n)
rownames(dtm)<-data$firm

dtm2<-as.matrix(mydfm_n2)

dtmw<-as.matrix(mydfmw_n)
dtmw2<-as.matrix(mydfmw_n2)
```



## HALS 

@cichocki2007 "Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization" and @degleris2019 "Fast Convolutive Nonnegative Matrix Factorization
Through Coordinate and Block Coordinate Updates".

\[J(U,V,Q) = ||M-U^\top V||^2 + \lambda ||C-U^\top Q|| = ||M-\sum_{k=1}^K u_kv_k^\top||^2 + \lambda ||C-\sum_{k=1}^K u_kq_k^\top||\\
=||M-\sum_{k\not=p} u_kv_k^\top - u_pv_p^\top||^2 + \lambda ||C-\sum_{k\not=p} u_kq_k^\top - u_pq_p^\top||\\
= Tr((M-\sum_{k\not=p} u_kv_k^\top)^\top (M-\sum_{k\not=p} u_kv_k^\top) - 2(M-\sum_{k\not=p} u_kv_k^\top)u_pv_p^\top + u_pv_p^\top v_p u_p) + \\
+\lambda Tr((C-\sum_{k\not=p} u_kq_k^\top)^\top (C-\sum_{k\not=p} u_kq_k^\top) - 2(C-\sum_{k\not=p} u_kq_k^\top)u_pq_p^\top + u_pq_p^\top q_p u_p)\]

The derivative with respect to $u_p$ is:

\[\frac{\partial J(U,V,Q)}{\partial u_p} = - 2(M-\sum_{k\not=p} u_kv_k^\top)v_p^\top + 2u_pv_p^\top v_p - 2\lambda (C-\sum_{k\not=p} u_kq_k^\top)q_p^\top + 2\lambda u_pq_p^\top q_p\]

Hence with Karush-Kuhn-Tucker conditions for optimality:

\[u_p = \max\left(0, \frac{(M-\sum_{k\not=p} u_kv_k^\top)v_p^\top + \lambda (C-\sum_{k\not=p} u_kq_k^\top)q_p^\top)}{v_p^\top v_p + \lambda q_p^\top q_p}\right)\]

The update rules for $v_p$ and $q_p$ do not differ from the HALS algorithm for NMF in Cochocki 2007.

\[v_p = \max\left(0, \frac{u_p(M-\sum_{k\not=p} u_kv_k^\top)}{u_p^\top u_p}\right)\]

\[q_p = \max\left(0, \frac{u_p(C-\sum_{k\not=p} u_kq_k^\top)}{u_p^\top u_p}\right)\]


define matrices to factorize


```{r}
what<-t(dtm) # ppmii
what2<-t(dtm2) #ppmii2
```

choose K

```{r}
#choosing K bei screeplot
evals<-irlba::partial_eigen(what, n = 20, symmetric = F)
evals2<-suppressWarnings(irlba::partial_eigen(what2, n = 15, symmetric = F))
par(mfrow=c(1,2))
plot(evals$values, type="l")# K=5 seems ok
plot(evals2$values, type="l")# K=5 seems ok
```



fit matrix factorization with 10 topics

```{r}

# #out<-compute_proj(M=what, C=what2,K=6,Kt=2,lam=500)
# out$losses[1]/out$losses[2]
# out$props

ind<-apply(what,1,sum)>0
out<-compute_proj0(M=what[ind,], K=8)
colnames(out$U)<-rownames(what[ind,])
save(out, file="out_k8_lam0.RData")
```
prepare coherence

```{r}
#compute cooccurrences
tcm = crossprod(sign(as.matrix(mydfm_n)))
tcm2 = crossprod(sign(as.matrix(mydfm_n2)))

#smooth otherwise :0
smooth<-10^(-12)# 10^(-5)
diag(tcm)<-diag(tcm)+smooth
diag(tcm2)<-diag(tcm2)+smooth
```


 topics
 
```{r}
U=out$U
tnames<-character(nrow(U))
for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
  tnames[i]<-paste(rownames(what[ind,])[order(U[i,],decreasing=T)][1:3], collapse=" ")
}


```
 project the goals on those topics
 
```{r}
project<-function(newdata, basis){
  M<-newdata
  U<-basis
  V<-solve((U)%*%t(U))%*%(U)%*%(M); V[V<0]<-0
    for(p in 1:nrow(U)){
      V[p,]<-update_v_p(p=p,M=M,U=U,V=V)
    }
  return(V)
}
Vp<-project(what2[ind,],out$U)
Vs<-Vp/matrix(apply(Vp,2,sum),dim(Vp)[1],dim(Vp)[2],byrow=T)
```

```{r}
nms<-unlist(strsplit(rownames(t(what2)),"_")); nms<-nms[grepl(".pdf",nms)]; nms<-gsub(".pdf","",nms) #c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
par(mfrow=c(3,3),mar=rep(2,4))

for(i in 2:nrow(U)){
  nums<-c(1,i)
  plot(c(Vs[nums[1],]),c(Vs[nums[2],]),col=NA,ylim=c(-0.1,1),xlab="",ylab="",main=paste("topic",i,":",tnames[i]))
  abline(h=0,lty=2)
  text(c(Vs[nums[1],]),c(Vs[nums[2],]),nms)
}

```
compute coherence

```{r}
 topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
    }
  cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
  cohi[cohi==0]<-NA
  mcohi<-apply(cohi,2, mean,na.rm=T)
c(mcohi, mean(mcohi))

```


 fit matrix co-factorization with 10 topics

```{r}
ind<-TRUE
lam=100#225
out<-compute_proj(M=what, C=what2,K=8,lam=lam) # K=9, lam=200
#out$losses[1]/(out$losses[2]*lam)
#out$props

#out$losses[1]/prod(dim(what))
#out$losses[2]/prod(dim(what2))
```


 topics
 
```{r}
U=out$U
tnames<-character(nrow(U))

for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
  tnames[i]<-paste(rownames(what[ind,])[order(U[i,],decreasing=T)][1:3], collapse=" ")
}


```
 
 
adjust scores on 17 goals to sum up to one over the topics -> can see the relatedness

```{r}
Q<-out$Q
Qs<-Q/matrix(apply(Q,2,sum),dim(Q)[1],dim(Q)[2],byrow=T)
```



```{r}
nms<-unlist(strsplit(rownames(t(what2)),"_")); nms<-nms[grepl(".pdf",nms)]; nms<-gsub(".pdf","",nms) #c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
par(mfrow=c(3,3),mar=rep(2,4))

for(i in 2:nrow(U)){
  nums<-c(1,i)
  plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA,ylim=c(-0.1,1),xlab="",ylab="",main=paste("topic",i,":",tnames[i]))
  abline(h=0,lty=2)
  text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
}

```

```{r}
 topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
    }
  cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
  cohi[cohi==0]<-NA
    mcohi<-apply(cohi,2, mean,na.rm=T)
c(mcohi, mean(mcohi))


```


A data-driven procedure to choose $K$ and $\lambda$:

- Choose candidate values of $K$ by inspecting the scree plots (5-10). Maybe 4-10? 

- Choose candidate values of $\lambda$ based on the zero model loss ratios: 1:1 to 10:1.

- Choose the optimal $K$ and $\lambda$ by average coherence.



```{r}
Ks<-seq(5,15)
lams<-seq(0,700,50)

```


compute coherence

```{r}
# coh<-coh1<-coh2<-mse<-prop<-prop2<-matrix(NA,length(Ks), length(lams))
# for (k in 1:length(Ks)){
#   for(l in 1:length(lams)){
#     print(c(k,l))
#     outt<-compute_proj(M=what, C=what2, K=Ks[k],lam=lams[l])
#     U<-outt$U
# 
#     topwords<-NULL
#     for(i in 1:nrow(U)){
#       topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
#     }
#     cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
#     cohi[cohi==0]<-NA
#     cohi<-apply(cohi,2,mean,na.rm=T)
#     coh[k,l]<-mean(cohi)
#     coh1[k,l]<-cohi[1]
#     coh2[k,l]<-cohi[2]
#     mse[k,l]<-(outt$losses[1])+(outt$losses[2])
#     prop[k,l]<-outt$props[1]
#     prop2[k,l]<-outt$props[2]
#   }
# }
# 
# cohs<-list(coh=coh, coh1=coh1, coh2= coh2,mse=mse,prop=prop,prop2=prop2)
# # save(cohs, file="cohs_new.RData")
# save(cohs, file="cohs_tech.RData")
```
 
 more precise lambda
 
```{r}
# #lamss<-seq(300,400,10)
# lamss<-seq(340,360,1)
# #
# Ks=8
# coh<-numeric(length(lamss))
# 
# for(l in 1:length(lamss)){
#     print(c(k,l))
#     outt<-compute_proj(M=what, C=what2, K=Ks,lam=lamss[l])
#     U<-outt$U
# 
#     topwords<-NULL
#     for(i in 1:nrow(U)){
#       topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
#     }
#     cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
#     cohi[cohi==0]<-NA
#     cohi<-apply(cohi,2,mean,na.rm=T)
#     coh[l]<-mean(cohi)
# }
# 
# #save(coh, file="coh_tech.RData")
# plot(coh)
# coh[11:12]
```
 

```{r}
load("cohs_tech.RData")
cohs$coh
# plot(lams,cohs$coh[1,], type="l",col=1, ylim=c(-5,-2))
# for (i in 2:(length(Ks)-5)){
#   lines(lams,cohs$coh[i,],col=i)
# }
plot(lams,cohs$coh[which(Ks==8),], type="l",col=1, ylim=c(-5,-1))

#which(cohs$coh==max(cohs$coh),arr.ind=T)
#cohs$coh[which(cohs$coh==max(cohs$coh),arr.ind=T)]

opt_lam<-lams[apply(cohs$coh,1,which.max)]
#plot(Ks,opt_lam, type="l")
```


```{r}
mse = cohs$mse; prop = cohs$prop; prop2 = cohs$prop2
coh = cohs$coh


#which(mse==min(mse),arr.ind = T)

popp<-matrix(0,dim(prop)[1],dim(prop)[2])
pthr<-c(0.2,0.5)
popp[prop>pthr[1]&prop2>pthr[2]]<-(prop2/prop)[prop>pthr[1]&prop2>pthr[2]]
#popp
#prop+prop2

rownames(coh)<-Ks
colnames(coh)<-lams
#coh
ind_best<-c(which(coh==max(coh),arr.ind = T))#-2.7218
lams[ind_best[2]] #400
Ks[ind_best[1]] #6

#load("cohs.RData")
#cohs$prop

```


 fit matrix co-factorization with 8 topics

```{r}
ind<-TRUE
lam=lams[ind_best[2]]
#fit
out<-compute_proj(M=what, C=what2,K=Ks[ind_best[1]],lam=lam) # K=9, lam=200
#colnames
colnames(out$U)<-rownames(what[ind,])
out$Qs<-Q/matrix(apply(Q,2,sum),dim(Q)[1],dim(Q)[2],byrow=T)
coln<-sapply(colnames(Q),function(x,split="/"){return(strsplit(x,split=split)[[1]][4])})
coln<-substr(coln,1,nchar(coln)-4)
colnames(out$Q)<-colnames(out$Qs)<-coln
#topic names
tnames<-character(nrow(U))
for( i in 1:nrow(U)){
  tnames[i]<-paste(rownames(what[ind,])[order(U[i,],decreasing=T)][1:3], collapse=" ")
}
out$tnames<-tnames
#V
V<-out$V
coln<-sapply(colnames(V),function(x,split="/"){return(strsplit(x,split=split)[[1]][3])})
coln<-substr(coln,1,nchar(coln)-4)
for(i in 1:length(coln)){
  if(grepl("_s",coln[i])){
    coln[i]<-substr(coln[i],1,nchar(coln[i])-2)
  }
}
colnames(V)<-coln
Vs<-aggregate(t(V),by=list(colnames(V)), FUN=function(x){max(x[x>0])})
colv<-Vs$Group.1
Vs<-t(Vs[,-1])
colnames(Vs)<-colv
out$Vs<-Vs
#save
save(out,file="out_k8_lam350.RData")
```


 topics
 
```{r}
U=out$U

for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
}


```


adjust scores on 17 goals to sum up to one over the topics -> can see the relatedness

```{r}
Q<-out$Q
Qs<-out$Qs
```



```{r}
nms<-unlist(strsplit(rownames(t(what2)),"_")); nms<-nms[grepl(".pdf",nms)]; nms<-gsub(".pdf","",nms) #c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
par(mfrow=c(3,3),mar=rep(2,4))

for(i in 1:nrow(U)){
  nums<-c(1,i)
  plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA,ylim=c(-0.1,1),xlab="",ylab="",main=paste("topic",i,":",tnames[i]))
  abline(h=0,lty=2)
  text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
}

```

```{r}
 topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
    }
  cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
  cohi[cohi==0]<-NA
    mcohi<-apply(cohi,2, mean,na.rm=T)
c(mcohi, mean(mcohi))


```

## Visualizing the results

compute the distance of each report page to each goal

```{r}
V<-out$V
Q<-out$Q
compute_dist<-function(x0,x1){
  dist<-sqrt(sum((x0-x1)^2))
  return(dist)
}
compute_col_dist<-function(x0mat,x1){
  dist<-apply(x0mat,2,compute_dist,x1=x1)
  return(dist)
}
compute_all_dist<-function(x0mat,x1mat){
  dist<-apply(x1mat,2,compute_col_dist,x0mat=x0mat)
  return(dist)
}
dist<-compute_all_dist(V,Q)
#head(dist)

rows_dist<-sapply(rownames(dist),function(x,split="/"){return(strsplit(x,split=split)[[1]][3])})
rows_dist<-substr(rows_dist,1,nchar(rows_dist)-4)

rownames(dist)<-rows_dist
```

compute the minimum distance

```{r}
reports<-unique(rownames(dist))
#reduce reports name with _s to firm_year
for(i in 1:length(reports)){
  if(grepl("_s",reports[i])){
    reports[i]<-substr(reports[i],1,nchar(reports[i])-2)
  }
}
reports_u<-reports
reports<-reports[!duplicated(reports)]


min_dist<-matrix(NA,length(reports), ncol(dist))
for(i in 1:length(reports)){
  disti<-dist[rownames(dist)==reports[i],]
  min_dist[i,]<-apply(disti,2,min)
}

rownames(min_dist)<-reports
colnames(min_dist)<-colnames(dist)
min_dist<-min_dist[,match(paste0("Goal_",1:17),colnames(min_dist))]
head(min_dist)

```

normalize the distances

```{r}
min_dist_norm<-min_dist/matrix(apply(min_dist,2,max),dim(min_dist)[1],dim(min_dist)[2],byrow = T)
min_dist_norm<-min_dist_norm[,match(paste0("Goal_",1:17),colnames(min_dist_norm))]
head(min_dist_norm)
save(min_dist_norm,file="min_dist_norm.RData")
```

```{r}
library(scales)
#parallel coordinate plot
#names<-c("BASF","IKEA","MS","Nestle","Toyota","Walmart")
names<-unique(sapply(reports,function(x,split="_"){return(strsplit(x,split=split)[[1]][1])}))

ind<-matrix(NA,nrow(min_dist_norm),length(names))
cols<-rep(1,nrow(min_dist_norm))
for(i in 2:length(names)){
 cols[grepl(names[i],rownames(min_dist_norm))]<-i
}
satur<-rep(seq(0.1,1,0.1),times=length(names))

plot(1:ncol(min_dist_norm),min_dist_norm[1,],col=NA,ylim=c(0,1),axes=F,ylab="norm. distance",xlab="")
for(i in 1:nrow(min_dist_norm)){
  lines(1:ncol(min_dist_norm),min_dist_norm[i,],col=alpha(cols[i],satur[i]))
}
axis(2)
axis(1,at=1:17,labels=paste0("G",1:17),cex.axis=0.5)
legend(1,0.1,col=1:2,lty=1,legend=names[1:2],cex=0.7, bty="n")
legend(4,0.1,col=3:4,lty=1,legend=names[3:4],cex=0.7, bty="n")
legend(7,0.1,col=5:6,lty=1,legend=names[5:6],cex=0.7, bty="n")
legend(10,0.1,col=7:8,lty=1,legend=names[7:8],cex=0.7, bty="n")

```

```{r}
image(t(min_dist_norm), oldstyle = T, axes=F)

coords<-seq(1,6,1)/6
coords2<-c(0,seq(1,17,1)/17)
names2<-paste0("G",1:17)

par(xpd=T)
for(i in 1:length(coords)){
  segments(-0.05,coords[i],1.05,coords[i])#add line
  text(-0.1,coords[i]-0.05,names[i])
}
for(i in 1:length(coords2)){
  text(coords2[i]*1.07,1.1,names2[i])
}
```


compute cosine similarity

```{r}
cos.sim <- function(ix) 
{
    A = V[,ix[1]]
    B = Q[,ix[2]]
    return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}   
n <- ncol(V)
m<- ncol(Q)
cmb <- expand.grid(i=1:n, j=1:m) 
C <- matrix(apply(cmb,1,cos.sim),n,m)

rows_dist_n<-rows_dist
for(i in 1:length(rows_dist)){
  if(grepl("_s",rows_dist_n[i])){
    rows_dist_n[i]<-substr(rows_dist_n[i],1,nchar(rows_dist_n[i])-2)
  }
}

rownames(C)<-rows_dist_n
colnames(C)<-colnames(min_dist)

reports<-unique(rows_dist_n)
min_dist2<-matrix(NA,length(reports), ncol(dist))
i=2
for(i in 1:length(reports)){
  disti<-C[rownames(C)==reports[i],]; disti<-apply(disti,2,function(x){x[!is.nan(x)]})
  min_dist2[i,]<-apply(disti,2,max)
}
rownames(min_dist2)<-reports
colnames(min_dist2)<-colnames(dist)
head(min_dist2)

cnames<-paste0("Goal_",1:17)
min_dist<-min_dist[,match(colnames(min_dist2),cnames)]



image(t(min_dist2),oldstyle=T,col=RColorBrewer::brewer.pal(9,"YlGn"),axes=F)
par(xpd=T)
for(i in 1:length(coords)){
  segments(-0.05,coords[i],1.05,coords[i])#add line
  text(-0.1,coords[i]-0.05,names[i])
}
for(i in 1:length(coords2)){
  text(coords2[i]*1.07,1.1,names2[i])
}


#apply(min_dist,2,mean)
min_dist2[1:10,]
```

For each goal:

- find the two most informative PCs
- plot the firms around


```{r}
inf_comps<-matrix(NA,17,2)
for(i in 1:17){
  qs<-Qs[,paste0("Goal_",i)]
  inf_comps[i,1]<-which.max(qs)
  qs[inf_comps[i,1]]<-0
  inf_comps[i,2]<-which.max(qs)
}
save(inf_comps,file="inf_comps.RData")
```

preprocess V, determine the page with the closest distance
```{r}
V<-out$V
#head(colnames(V))
Vs<-out$Vs

```


```{r}
i<-4 #1,4,9,12
colnames(Q)<-colnames(Qs)
ymax<-max(c(Q[inf_comps[i,2],paste0("Goal_",i)],Vs[inf_comps[i,2],]))
xmax<-max(c(Q[inf_comps[i,1],paste0("Goal_",i)],Vs[inf_comps[i,1],]))
plot(Q[inf_comps[i,1],paste0("Goal_",i)], Q[inf_comps[i,2],paste0("Goal_",i)], xlab=paste("Topic", inf_comps[i,1]),ylab=paste("Topic", inf_comps[i,2]),col=NA,
     ylim=c(0,ymax), xlim=c(0,xmax))
text(Q[inf_comps[i,1],paste0("Goal_",i)], Q[inf_comps[i,2],paste0("Goal_",i)], paste0("Goal_",i), col=2)
#points(Vs[inf_comps[i,1],], Vs[inf_comps[i,2],])
text(Vs[inf_comps[i,1],], Vs[inf_comps[i,2],],colnames(Vs),cex=0.5,col =scales::alpha("gray44",0.5))
```


make gifs ?


```{r}
i<-4 #1,4,9,12
what<-"AMZ"
plot(Q[inf_comps[i,1],paste0("Goal_",i)], Q[inf_comps[i,2],paste0("Goal_",i)], xlab=paste("Topic", inf_comps[i,1]),ylab=paste("Topic", inf_comps[i,2]),col=NA,
     ylim=c(40,250), xlim=c(0,250))
text(Q[inf_comps[i,1],paste0("Goal_",i)], Q[inf_comps[i,2],paste0("Goal_",i)], paste0("Goal_",i), col=2)
#points(Vs[inf_comps[i,1],], Vs[inf_comps[i,2],])
text(Vs[inf_comps[i,1],grepl(what,colnames(Vs))], Vs[inf_comps[i,2],grepl(what,colnames(Vs))],colnames(Vs)[grepl(what,colnames(Vs))],cex=0.5)
```


Considering individual preferences

Take a linear combination of the goals with weights $\beta=(\beta_1,\ldots,\beta_{17})^\top$ then $C\beta\approx UQ^\top\beta$ defines a "personalized" goal in terms of term occurrences approximated by the factorization.

```{r}
weights<-c(10,0,0,0,0,0,3,0,0,4,0,0,5,0,0,0,0)
weights<-weights/sum(weights)
b<-matrix(weights,,1)
mygoal<-t(U)%*%Q%*%b

#score
myscore<-Q%*%b

#most informative
myscore0<-myscore
my_comps<-which.max(myscore0)
myscore0[my_comps]<-0
my_comps<-c(my_comps,which.max(myscore0))

```

```{r}
#year
years = unique(sapply(colnames(Vs), function(x){substr(x,nchar(x)-3, nchar(x))}))
year = 2024
firms = sapply(colnames(Vs), function(x){strsplit(x,split="_")[[1]][1]})

if(year%in%years){
  Vsy<-Vs[,grepl(year,colnames(Vs))]
} else {
  Vsy<-aggregate(t(Vs), by=list(firms),mean)
  rownames(Vsy)<-Vsy[,1]
  Vsy<-t(Vsy[,-1]); Vsy<-as.matrix(Vsy)
}

```


```{r}
par(xpd=T)
ymax<-max(c(myscore[my_comps[2]],Vsy[my_comps[2],]))
xmax<-max(c(myscore[my_comps[1]],Vsy[my_comps[1],]))
plot(myscore[my_comps[1]], myscore[my_comps[2]], xlab=paste("Topic", my_comps[1]),ylab=paste("Topic", my_comps[2]),col=NA,
     ylim=c(0,ymax), xlim=c(0,xmax))
text(myscore[my_comps[1]], myscore[my_comps[2]], paste0("my goal"), col=2)
text(Vsy[my_comps[1],], Vsy[my_comps[2],],colnames(Vsy),cex=0.5)
```

compute the distances

```{r}
mydist<-compute_col_dist(V,myscore)
min_dist<-aggregate(as.matrix(mydist),by=list(names(mydist)), min)
mynames<-min_dist[,1]
min_dist<-min_dist[,-1]; names(min_dist)<-mynames
```

```{r}
year = 2022
if(year%in%years){
 min_disty<-min_dist[grepl(year,names(min_dist))]
} else {
  min_disty<-aggregate(as.matrix(min_dist), by=list(firms),mean)
  mynames<-min_disty[,1]
  min_disty<-min_disty[,-1]; names(min_disty)<-mynames
}
names(min_disty)[order(min_disty)]


```

