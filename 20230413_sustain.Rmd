---
title: "Identifying sustainability dimensions of corporate reports by directed topic analysis"
author: "Maria Osipenko"
date: '2023-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Idea

-	Topic analysis represents each document in a collection of documents in a low dimensional latent topic space

- Supervised and unsupervised approaches are possible. 

- Due to limitations of supervised techniques (costs for labeling, subjectivity) we use an unsupervised methodology for embedding in a low dimensional space.

- However, sometimes the latent space is prestructured by e.g. policy maker as some sort of regulation requirement, on which the documents are matched. 

- We model the later given structure by stratifying to match the occurrences of terms with a  specific keyword collection from the regulatory texts. In the case of sustainability reports - 17 sustainability goals.

- As a result, a low dimensional representation of phrases in the report sentences and the sentences themselves stratified by the defined sustainability goals is obtained.

- Use data from @kang2022

For a comprehensive review of text mining in finance literature, we refer to @gupta2020.

@Harandizadeh2022 proposes to use word2vec embeddings combined with LDA and vocabulary priors to obtain interpretable word embeddings.
@eshima2023 embedd prespecified keywords in LDA for the same reason. In the same spirit, @watanabe2022 use seeded LDA with a carefuly chosen seeded vocabulary to assist in classifying documents in specific categories. With their approaches, the authors embedd additional information in topic extraction.

The draw back of the mentioned approaches in @watanabe2022 and @eshima2023 lies in the need of manual intervention for keyword or vocabulary specification. @Harandizadeh2022 uses word vectors from a pretrained general purpose word2vec model and thus, it is not clear, whether their model works for specific domains as sustainability reports.

Another way to consider such additional information are matrix co-factorization techniques, which are especially popular in recommended systems. Co-factorization techniques factorize two or three matrices with some common cofactors simultaneously. For instance, @fang2011 consider user communities information and @luo2019 incorporate tagging and time stamp of ratings in their personalized recommendations via matrix co-factorization. The approach is transparent and easily adjustable. By introducing a nuisance parameter which allows to move focus between error minimization of individual factorization terms, additional flexibility is ensured.

For this reasons, we follow the later approach and propose to use matrix co-factorization for embedding additional information from sustainable goals definition into topics extraction from corporate reports.

## Model

We assume that the corporate reports texts share a common topic structure with the sustainability goals definition but also contain some other topics concerning e.g. financial statements. Moreover, we anticipate that the goals are written very focused using concrete sparse vocabulary, whereas the reports may refer to the same concepts using other wordings. That is, a common topic may contain words that are semantically relevant to sustainability goals vocabulary but not directly mentioned in the texts of the SDGs.
That is, we assume, that both text corpora share a common low dimensional subspace, in which they can be compared to each other by means of some distance measure.

To account for the mentioned issues, we define the following model for terms-document matrices arising from reports  and sustainability goals texts.

\[M = U^\top V + E\]

and 

\[C = {U_{1:k}}^\top Q + F\]

where $U_{1:k}$ is a matrix which contains only the first $k$ rows of $U$ and

- $M$ is the term-document matrix for the corporate reports with dimensions $(p\times n)$, where $p$ is the joint vocabulary (words and phrases with two co-occurring words) obtained from both reports and sustainability goals texts. $n$ is the number of corporate reports contexts, where the later represents one page of a corporate report. The overall dimensions for $M$ for our data are $(18'086\times 6'891)$.
- $C$ is the term-document matrix for the sustainability goals with dimensions $(p\times m)$, where $p$ is again the joint vocabulary (words and phrases with two co-occurring words) obtained from both reports and sustainability goals texts. $m$ is the number of sustainability goals contexts, where each context represents each of the $17$ goals. The overall dimensions for $C$ for our data are $(18'086\times 17)$.
- $U$ is the word-topic representation matrix of dimensions $(p\times k+\tilde k)$, where $k$ is the number of common topics and $\tilde k$ is the number of report specific topics.
- $V$ is the context-topic representation matrix for the reports of dimensions $(k+\tilde k\times n)$.
- $Q$ is the context-topic representation matrix for sustainability goals of dimensions $(k\times m)$.
- $E$ and $F$ are matrices of error terms of dimensions $(p\times n)$ and $(p\times m)$ respectively.

The associated topic extraction problem is then:

\[\min(||M - U^\top V||^2 + \lambda ||C-U_0^\top Q||^2)\]

where $\lambda$ adapts the importance of the loss on the second factorization term.

Because of the non-negativity of the entries in $M$ and $C$ it makes sense to restrict at least $U$ to be non-negative. This enhances the interpretability and sparsity of the resulting topics **(see cite)**.

The value of $\lambda$ balances out the combined loss function. It is responsible for adjusting the impact of accuracy concerning reports versus SDGs. Since the second dimension of $C$ is much lower than that of $M$, the first part of the loss will dominate the co-factorization. To give more weight to the second part one can alternate $\lambda$.

In summary, with the resulting low dimensional representation of corporate reports together with SDGs, we create a basis for choosing, evaluating and monitoring investments with respect to their impact on society and the environment.

## Simulation

```{r}

```


## Data 

https://github.com/llbtl/paper_ssm01/tree/main/data

```{r}
rm(list=ls())
```



```{r}
# files<-list.files("data_pdf/reports",full.names = T)
# 
# contents<-paragraphs<-data.frame()
# 
# for (f in 1:length(files)){
#     #print(files[f])
#     content<-Rpoppler::PDF_text(files[f])
#     contents<-rbind(contents, data.frame(text=content,firm=rep(files[f],length(content))))
#     paragraph<-unlist(strsplit(content, "\n[0-9]"))#too long
#     paragraph<-paragraph[nchar(paragraph)>300]
#     paragraphs<-rbind(paragraphs, data.frame(text=paragraph,firm=rep(files[f],length(paragraph))))
# }
# 
# preproc_content<-function(contents,thr=300){
#   contents<-contents[nchar(as.character(contents[,1]))>thr,]
#   contents[,1]<-gsub(pattern="\n[0-9]",replacement=" ",x=as.character(contents[,1]))
#   contents[,1]<-gsub(pattern="-\n",replacement="",x=as.character(contents[,1]))
#   contents[,1]<-gsub(pattern="\n",replacement=" ",x=as.character(contents[,1]))
#   return(contents)
# }
# 
# contents<-preproc_content(contents)
# paragraphs<-preproc_content(paragraphs)
# 



```


```{r}
# data<-contents
# data$text<-as.character(data$text)
# save(data, file="data_sustain.RData")
# 
# data<-paragraphs
# data$text<-as.character(data$text)
# save(data, file="data_sustain_paragr.RData")
```

```{r}
#load("data_sustain_paragr.RData")
load("data_sustain.RData")
```


## Data

Use R-Package Quanteda (Benoit K, Watanabe K, Wang H, Nulty P, Obeng A, Müller S, Matsuo A (2018). “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software, 3(30), 774. doi: 10.21105/joss.00774, https://quanteda.io.) to set up a corpus, to split in tokens, and compute the relative frequencies.

corporate reports

```{r, warnings=F, message=FALSE}
library(quanteda)
data$id<-1:nrow(data)
mycorpus<-corpus(data, docid_field="id",text_field="text")
print(paste("Number of docs is",ndoc(mycorpus)))
#tokenize
tk<-tokenizers::tokenize_word_stems(data[,1], stopwords = c(stopwords::stopwords("en"),"toyota", "walmart","nestle","basf","ikea","ms","m&s",2011:2020))
tk<-tokens(as.tokens(tk), remove_punct=TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens=T)
tk2<-tokens_ngrams(as.tokens(tk),n=1:2,skip=2)
#dfm
# mydfm<-dfm(tk2, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
mydfm<-dfm(tk, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()


words2remove<-c("[[:digit:]]","toyota", "walmart","nestle","basf","ikea","ms","m&s","per", "wal", "mart","nestlé","bayer","www.sdgcompass.org", "e.g","i’m","said",
                "ulrich", "han", "wayn", "heinz", "smith", "brudermül", "kurt", "suckal", "margret","harald", "gandhi", "sanjeev",  "michael", "andrea",
                "franz", "diekmann", "jürgen" , "hambrecht" ,   "schäferkordt","robert","e.u" ,"kpmg" )

#keep only terms with more than three chars
mydfm<-dfm_select(mydfm,pattern=words2remove, valuetype="regex",selection="remove",min_nchar = 3)
mydfm<-dfm_select(mydfm,pattern=stopwords::stopwords("en"),valuetype="fixed",selection="remove")
#keep only words with frequency >=10
mydfm<-dfm_trim(mydfm, min_docfreq = 0.005, max_docfreq=0.5,docfreq_type="prop")
#take relative frequencies as entries
#mydfm_w<-dfm_weight(mydfm, scheme = "prop")
mydfm
topfeatures(mydfm)
```

sustainability goals (https://www.un.org/sustainabledevelopment/)

```{r}
# files<-list.files("data_pdf/SDGs",full.names = T,recursive = T)
# 
# contents<-data.frame()
# 
# for (f in 1:length(files)){
#     #print(files[f])
#     content<-Rpoppler::PDF_text(files[f])
#     contents<-rbind(contents, data.frame(text=content,firm=rep(files[f],length(content))))
# }
# contents<-contents[nchar(as.character(contents[,1]))>300,]
# contents[,1]<-gsub(pattern="\n[0-9]",replacement=" ",x=as.character(contents[,1]))
# contents[,1]<-gsub(pattern="-\n",replacement="",x=as.character(contents[,1]))
# contents[,1]<-gsub(pattern="\n",replacement=" ",x=as.character(contents[,1]))
```

```{r}
# goals<-contents
# #goals$text<-as.character(data$text)
# save(goals, file="goals_sustain.RData")
```

```{r}
load("goals_sustain.RData")
```


```{r, warnings=F, message=FALSE}
library(quanteda)
goals$id<-1:nrow(goals)
mycorpus<-corpus(goals, docid_field="id",text_field="text")
print(paste("Number of docs is",ndoc(mycorpus)))
#tokenize
tkk<-tokenizers::tokenize_word_stems(goals[,1], stopwords = c(stopwords::stopwords("en")))
tkk<-tokens(as.tokens(tkk), remove_punct=TRUE, remove_numbers = TRUE, remove_symbols = TRUE,remove_url = TRUE, remove_separators = TRUE, split_hyphens=T)
tk22<-tokens_ngrams(as.tokens(tkk),n=1:2,skip=2)
#dfm
# mydfm2<-dfm(tk22, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
mydfm2<-dfm(tkk, verbose = FALSE,tolower=TRUE)%>%dfm_wordstem()
#keep only terms with more than two chars
mydfm2<-dfm_select(mydfm2,pattern=words2remove, valuetype="regex",selection="remove",min_nchar = 3)
mydfm2<-dfm_select(mydfm2,pattern=stopwords::stopwords("en"),valuetype="fixed",selection="remove")
#keep only words with frequency >=2
mydfm2<-dfm_trim(mydfm2, min_docfreq = 0.01, max_docfreq=0.5,docfreq_type="prop")
#take relative frequencies as entries
#mydfm_w<-dfm_weight(mydfm, scheme = "prop")
mydfm2

mydfm2<-dfm_group(mydfm2,goals$firm)
```


combine the features

```{r}
myfeatures<-c(featnames(mydfm), featnames(mydfm2))
myfeatures<-myfeatures[!duplicated(myfeatures)]
```


```{r}
mydfm_n<-dfm_match(mydfm, features=myfeatures)
mydfmw_n<-dfm_tfidf(mydfm_n)
mydfm_n2<-dfm_match(mydfm2, features=myfeatures)
mydfm_n2
mydfmw_n2<-dfm_tfidf(mydfm_n2)
```

convert to matrix


```{r}
## export as matrix
dtm<-as.matrix(mydfm_n)
rownames(dtm)<-data$firm

dtm2<-as.matrix(mydfm_n2)

dtmw<-as.matrix(mydfmw_n)
dtmw2<-as.matrix(mydfmw_n2)
```


compute the joint probs $p(w,c)$  and the marginal probs $p(w)$.


```{r}
w_p<-matrix(apply(dtm,2,sum)/sum(dtm),dim(dtm)[1],dim(dtm)[2],byrow=T)#probability of word
c_p<-matrix(apply(dtm,1,sum)/sum(dtm),dim(dtm)[1],dim(dtm)[2],byrow=F)#probability of a firm content

w_p2<-matrix(apply(dtm2,2,sum)/sum(dtm2),dim(dtm2)[1],dim(dtm2)[2],byrow=T)#probability of word
c_p2<-matrix(apply(dtm2,1,sum)/sum(dtm2),dim(dtm2)[1],dim(dtm2)[2],byrow=F)#probability of a firm content

## in case of fcm
# w_p2<-matrix(apply(fcm2,2,sum)/sum(fcm2),dim(fcm2)[1],dim(fcm2)[2],byrow=T)#probability of word
# c_p2<-matrix(apply(fcm2,1,sum)/sum(fcm2),dim(fcm2)[1],dim(fcm2)[2],byrow=F)#probability of a firm content
```

compute ppmi:

\[ppmi_{wc} =\max\left(0,\log \frac{p(w,c)}{p(w)p(c)}\right)\]

```{r}
pmi<-log2((dtm/prod(dim(dtm)))/(w_p*c_p))
ppmi<-pmi; ppmi[ppmi<0]<-0
ppmi[is.infinite(ppmi)|is.nan(ppmi)]<-0

pmi2<-log2((dtm2/prod(dim(dtm2)))/(w_p2*c_p2))
ppmi2<-pmi2; ppmi2[ppmi2<0]<-0
ppmi2[is.infinite(ppmi2)|is.nan(ppmi2)]<-0

#indgo1<-apply(ppmi,1,function(x){sum(x!=0)})>10
#indgo2<-apply(ppmi,2,function(x){sum(x!=0)})>50 # omit words which occurred in less than 10 contents
#ppmii<-ppmi[indgo1,indgo2]

#indgo1<-apply(ppmi2,1,function(x){sum(x!=0)})>5
#indgo2<-apply(ppmi2,2,function(x){sum(x!=0)})>5 # omit words which occurred in less than 10 contents
#ppmii2<-ppmi2[indgo1,indgo2]
pmi<-t(pmi)
pmi2<-t(pmi2)

ppmii<-t(ppmi)
ppmii2<-t(ppmi2)

#ppmii<-ppmii/max(ppmii)
#ppmii2<-ppmii2/max(ppmii2)
```


## HALS 

@cichocki2007 "Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization" and @degleris2019 "Fast Convolutive Nonnegative Matrix Factorization
Through Coordinate and Block Coordinate Updates".

\[J(U,V,Q) = ||M-U^\top V||^2 + \lambda ||C-U^\top Q|| = ||M-\sum_{k=1}^K u_kv_k^\top||^2 + \lambda ||C-\sum_{k=1}^K u_kq_k^\top||\\
=||M-\sum_{k\not=p} u_kv_k^\top - u_pv_p^\top||^2 + \lambda ||C-\sum_{k\not=p} u_kq_k^\top - u_pq_p^\top||\\
= Tr((M-\sum_{k\not=p} u_kv_k^\top)^\top (M-\sum_{k\not=p} u_kv_k^\top) - 2(M-\sum_{k\not=p} u_kv_k^\top)u_pv_p^\top + u_pv_p^\top v_p u_p) + \\
+\lambda Tr((C-\sum_{k\not=p} u_kq_k^\top)^\top (C-\sum_{k\not=p} u_kq_k^\top) - 2(C-\sum_{k\not=p} u_kq_k^\top)u_pq_p^\top + u_pq_p^\top q_p u_p)\]

The derivative with respect to $u_p$ is:

\[\frac{\partial J(U,V,Q)}{\partial u_p} = - 2(M-\sum_{k\not=p} u_kv_k^\top)v_p^\top + 2u_pv_p^\top v_p - 2\lambda (C-\sum_{k\not=p} u_kq_k^\top)q_p^\top + 2\lambda u_pq_p^\top q_p\]

Hence with Karush-Kuhn-Tucker conditions for optimality:

\[u_p = \max\left(0, \frac{(M-\sum_{k\not=p} u_kv_k^\top)v_p^\top + \lambda (C-\sum_{k\not=p} u_kq_k^\top)q_p^\top)}{v_p^\top v_p + \lambda q_p^\top q_p}\right)\]

The update rules for $v_p$ and $q_p$ do not differ from the HALS algorithm for NMF in Cochocki 2007.

\[v_p = \max\left(0, \frac{u_p(M-\sum_{k\not=p} u_kv_k^\top)}{u_p^\top u_p}\right)\]

\[q_p = \max\left(0, \frac{u_p(C-\sum_{k\not=p} u_kq_k^\top)}{u_p^\top u_p}\right)\]

```{r}
update_u_p<-function(p,M,C,U,V,Q,lam,K){
  v_p<-V[p,,drop=F]
  sumC<-0; q_p<-0
  if(p<K){
    q_p<-Q[p,,drop=F]
    sumC<-(C-t(U[1:K,][-p,])%*%Q[-p,])%*%t(q_p)
  }
  
  sumM<-(M-t(U[-p,])%*%V[-p,])%*%t(v_p)
  u_p<-t((sumM+lam*sumC)/(sum((v_p)^2) + lam*sum((q_p)^2)))
  #t(u_p)
  #t(v_p)
  u_p[u_p<0]<-0
  u_p<-u_p/sum(u_p)
  return(u_p)
}

update_u_p0<-function(p,M,U,V,K){
  v_p<-V[p,,drop=F]
  sumM<-(M-t(U[-p,])%*%V[-p,])%*%t(v_p)
  u_p<-t((sumM)/(sum((v_p)^2)))
  #t(u_p)
  #t(v_p)
  u_p[u_p<0]<-0
  u_p<-u_p/sum(u_p)
  return(u_p)
}

# up<-update_u_p(p=1,M=ppmii,C=ppmii2,U=out$U,V=out$V,Q=out$Q,lam=0.1)
# U<-t(sapply(1:K,update_u_p,M=ppmii,C=ppmii2,U=out$U,V=out$V,Q=out$Q,lam=0.1))
# dim(U)
```

```{r}
update_v_p<-function(p,M,U,V){
  u_p<-U[p,,drop=F]
  sumM<-u_p%*%(M-t(U[-p,])%*%V[-p,])
  v_p<-((sumM)/(sum((u_p)^2)))
  #t(u_p)
  #t(sumM)
  #t(v_p)
  v_p[v_p<0]<-0
  #v_p<-v_p/sum(v_p)
  return(v_p)
}
# vp<-update_v_p(p=1,M=ppmii,U=out$U,V=out$V)
# dim(vp)
# V<-t(sapply(1:K,update_v_p,M=ppmii,U=out$U,V=out$V))

```

```{r}
update_q_p<-function(p,C,U,Q){
  u_p<-U[p,,drop=F]
  sumC<-u_p%*%(C-t(U[-p,])%*%Q[-p,])
  q_p<-t((sumC)/(sum((u_p)^2)))
  q_p[q_p<0]<-0
  #q_p<-q_p/sum(q_p)
  return(q_p)
}
# qp<-update_q_p(p=1,C=ppmii2,U=out$U,Q=out$Q)
# dim(qp)
# Q<-t(sapply(1:K,update_q_p,C=ppmii2,U=out$U,Q=out$Q))
```


define matrices to factorize


```{r}
what<-t(dtmw) # ppmii
what2<-t(dtmw2) #ppmii2
```

choose K

```{r}
#choosing K bei screeplot
evals<-irlba::partial_eigen(what, n = 20, symmetric = F)
evals2<-suppressWarnings(irlba::partial_eigen(what2, n = 10, symmetric = F))
par(mfrow=c(1,2))
plot(evals$values, type="l")# K=5 seems ok
plot(evals2$values, type="l")# K=5 seems ok
```

```{r}
# #parallel analyse
# S=10
# p<-dim(ppmii)[1]; n=dim(ppmii)[2]
# eigvals<-matrix(NA,15,S)
# m<-mean(ppmii[ppmii>0])
# sdd<-sd(ppmii[ppmii>0])
# sp<-sum(ppmii>0)/(prod(dim(ppmii)))
# for(s in 1:S){
#   print(s)
#   entries<-rep(0,n*p)
#   nzero<-sample.int(n*p,n*p*sp)
#   entries[nzero]<-sdd*rnorm(length(nzero))+m; 
#   entries<-matrix(entries,p,n)
#   eigvals[,s]<-irlba::partial_eigen(entries, n = 15, symmetric = F)$values
# }
# 
# eigvalss<-apply(eigvals,1,mean)
# plot(evals$values, type="l")# K=5 seems ok
# lines(eigvalss,col=2)
```

```{r}
compute_proj0<-function(M,K,thr=0.01,seed=1,iters=100){
  set.seed(seed)
  conv<-F
  i=0
  #initMat<-irlba::svdr(M,k=K)
  #U<-t(initMat$u)
  U=matrix(runif(dim(M)[1]*K),K,dim(M)[1])
  V=solve((U)%*%t(U))%*%(U)%*%(M); V[V<0]<-0
  loss<-1000
  print(K)
  while(!conv){
    i<-i+1
    
    
    for(p in 1:K){
      U[p,]<-update_u_p0(p=p,M=M,U=U,V=V)
      V[p,]<-update_v_p(p=p,M=M,U=U,V=V)
    }
    
    
    #check convergence
    loss<-c(loss, sum((M-t(U)%*%V)^2))
    losses<-c(sum((M-t(U)%*%V)^2))
    crit<-1000
    if(i>3){
      crit<-loss[length(loss)]-loss[length(loss)-1]
    }
    
    if(crit<thr|i>iters){
      conv<-TRUE
      }
    #print(loss[length(loss)])
  }
  #ratio of proportions of the explained variance
  props<-c((1-sum((M-t(U)%*%V)^2)/sum((M)^2)))
  prop<-props[1]/1
  out<-list(V=V,U=U,lam=0,prop=prop,props=props,loss=loss,losses=losses,iters=i)
  
  return(out)
}

compute_proj<-function(M,C,K,#common topics
                        Kt,#additional report specific topics
                        lam,thr=0.01,seed=1,iters=100){
  set.seed(seed)
  conv<-F
  i=0
  #initMat<-irlba::svdr(M,k=K+Kt)
  #U<-t(initMat$u)
  U=matrix(runif(dim(M)[1]*(K+Kt)),(K+Kt),dim(M)[1])
  V=solve((U)%*%t(U))%*%(U)%*%(M); V[V<0]<-0
  Q=solve((U[1:K,])%*%t(U[1:K,]))%*%(U[1:K,])%*%(C); Q[Q<0]<-0
  loss<-1000
  print(K)
  while(!conv){
    i<-i+1
    
    
    for(p in 1:K){
      U[p,]<-update_u_p(p=p,M=M,C=C,U=U,V=V,Q=Q,lam=lam,K=K)
      V[p,]<-update_v_p(p=p,M=M,U=U,V=V)
      Q[p,]<-update_q_p(p=p,C=C,U=U[1:K,],Q=Q)
    }
    
    if(Kt>0){
      for(p in (K+1):(K+Kt)){
        U[p,]<-update_u_p(p=p,M=M,C=C,U=U,V=V,Q=Q,lam=lam,K=K)
        V[p,]<-update_v_p(p=p,M=M,U=U,V=V)
      }
    }
    
    
    
    #check convergence
    loss<-c(loss, sum((M-t(U)%*%V)^2) + lam*sum((C-t(U[1:K,])%*%Q)^2))
    losses<-c(sum((M-t(U)%*%V)^2),sum((C-t(U[1:K,])%*%Q)^2))
    crit<-1000
    if(i>3){
      crit<-loss[length(loss)]-loss[length(loss)-1]
    }
    
    if(crit<thr|i>iters){
      conv<-TRUE
      }
    #print(loss[length(loss)])
  }
  #ratio of proportions of the explained variance
  props<-c((1-sum((M-t(U)%*%V)^2)/sum((M)^2)),(1-sum((C-t(U[1:K,])%*%Q)^2)/sum((C)^2)))
  prop<-props[1]/props[2]
  out<-list(V=V,U=U,Q=Q,lam=lam,prop=prop,props=props,loss=loss,losses=losses,iters=i)
  
  return(out)
}
```

fit matrix factorization with 10 topics

```{r}

# #out<-compute_proj(M=what, C=what2,K=6,Kt=2,lam=500)
# out$losses[1]/out$losses[2]
# out$props

ind<-apply(what,1,sum)>0
out<-compute_proj0(M=what[ind,], K=10)
```


 topics
 
```{r}
U=out$U

for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
}


```
 
 fit matrix co-factorization with 10 topics

```{r}
ind<-TRUE
out<-compute_proj(M=what, C=what2,K=8,Kt=2,lam=200)
# out$losses[1]/out$losses[2]
# out$props

```


 topics
 
```{r}
U=out$U

for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
}


```
 
 fit matrix co-factorization with 10 topics

```{r}
ind<-TRUE
lam=400#225
out<-compute_proj(M=what, C=what2,K=10,Kt=0,lam=lam) # K=9, lam=200
#out$losses[1]/(out$losses[2]*lam)
(out$losses[1]/prod(dim(what)))/(out$losses[2]/prod(dim(what2)))
#out$props

#out$losses[1]/prod(dim(what))
#out$losses[2]/prod(dim(what2))
```


 topics
 
```{r}
U=out$U
tnames<-character(nrow(U))

for( i in 1:nrow(U)){
  print(paste("Topic ",i))
  print(rownames(what[ind,])[order(U[i,],decreasing=T)][1:20])
  tnames[i]<-paste(rownames(what[ind,])[order(U[i,],decreasing=T)][1:3], collapse=" ")
}


```
 
 
adjust scores on 17 goals to sum up to one over the topics -> can see the relatedness

```{r}
Q<-out$Q
Qs<-Q/matrix(apply(Q,2,sum),dim(Q)[1],dim(Q)[2],byrow=T)
```



```{r}
nms<-unlist(strsplit(rownames(t(what2)),"_")); nms<-nms[grepl(".pdf",nms)]; nms<-gsub(".pdf","",nms) #c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
par(mfrow=c(3,3),mar=rep(2,4))

for(i in 2:nrow(U)){
  nums<-c(1,i)
  plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA,ylim=c(-0.1,1),xlab="",ylab="",main=paste("topic",i,":",tnames[i]))
  abline(h=0,lty=2)
  text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
}

```

A data-driven procedure to choose $K$ and $\lambda$:

- Choose candidate values of $K$ by inspecting the scree plots (5-10). Maybe 4-10? 

- Choose candidate values of $\lambda$ based on the zero model loss ratios: 1:1 to 10:1.

- Choose the optimal $K$ and $\lambda$ by average coherence.



```{r}
Ks<-seq(5,10)
from=0.001; to=1; fact=0.005
#lams<-seq(round((mean((what-mean(what))^2)/(mean((what2-mean(what2))^2)))/100)*100,round(mean((what-mean(what))^2)/(mean((what2-mean(what2))^2)*fact)),by=25)
lams<-seq(0,500,25)
```

prepare coherence

```{r}
#compute cooccurrences
tcm = crossprod(sign(as.matrix(mydfm_n)))
tcm2 = crossprod(sign(as.matrix(mydfm_n2)))

#smooth otherwise :0
smooth<-10^(-12)# 10^(-5)
diag(tcm)<-diag(tcm)+smooth
diag(tcm2)<-diag(tcm2)+smooth


# #http://qpleple.com/topic-coherence-to-evaluate-topic-models/
# compute_umass<-function(fcm,topwords){
#   umass<-function(fcm, topwords){
#     topwords<-topwords[topwords%in%rownames(fcm)]
#     coccmat<-fcm[topwords,topwords]
#     cooc<-mean(as.numeric(log((coccmat+1)/matrix(apply(coccmat,2,sum),length(topwords),length(topwords),byrow=T))))
#     return(cooc)
#   }
#   return(apply(topwords,1,umass,fcm=fcm))
# }
# 
# umass<-function(fcm, topwords){
#     topwords<-topwords[topwords%in%rownames(fcm)]
#     coccmat<-fcm[topwords,topwords]
#     #diag(coccmat)<-0
#     cooc<-mean(as.numeric(log((coccmat+0.0001)/matrix(apply(coccmat,2,sum),length(topwords),length(topwords),byrow=T))))
#     return(cooc)
# }
# 
# um<-NULL
# for(i in 1:nrow(topwords)){
#   um<-c(um,umass(fcm,topwords=topwords[i,]))
# }
# 
# compute_umass(myfcm,topwords)

```

compute coherence

```{r}
coh<-coh2<-matrix(NA,length(Ks), length(lams))
for (k in 1:length(Ks)){
  for(l in 1:length(lams)){
    print(c(k,l))
    outt<-compute_proj(M=what, C=what2, K=Ks[k],Kt=0,lam=lams[l])
    U<-outt$U

    topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,rownames(what)[order(U[i,],decreasing=T)][1:20])
    }
    cohi<-cbind(text2vec::coherence(topwords,tcm, metrics="mean_logratio"),text2vec::coherence(topwords,tcm2, metrics="mean_logratio"))
    cohi<-apply(cohi,1,mean)
    coh[k,l]<-mean(cohi)
  }
}
# cohm<-(coh+coh2)
# which(cohm==max(cohm), arr.ind=T)
# inds<-which(coh2==max(coh2), arr.ind=T)
# 
# Ks[inds[1]]
# lams[inds[2]]
# apply((coh+coh2),2,mean)
# apply((coh2),1,mean)
# lams[6]
#cohs<-list(coh=coh, coh2=coh2)

save(coh, file="coh.RData")

```


```{r}
#load("coh.RData")
coh
which(coh==max(coh),arr.ind = T)
```














```{r}

predict_proj<-function(K,M, C, lam,fold){
  out<-compute_proj(M=M[,-fold],C=C,lam=lam, K=K)
  U<-out$U
  Vpred<-solve((U)%*%t(U))%*%(U)%*%(M[,fold]); V[V<0]<-0
  preds<-t(U)%*%Vpred
  proppred<-1-(mean((M[,fold]-preds)^2))/(sd(M[,fold]))^2
  rmse<-sqrt(mean((M[,fold]-preds)^2))
  res<-list(proppred=proppred, rmse=rmse,prop=out$prop,props=out$props)
  return(res)
}

```

## Choose $k$ by cross validation on SDGs data

```{r}
Ks<-seq(3,10)
set.seed(1)
indgr0<-which(ppmii2>0, arr.ind=T); indgr0<-indgr0[sample.int(nrow(indgr0),replace=F),]
nfolds<-10
folds<-c(seq(1,nrow(indgr0),floor(nrow(indgr0)/nfolds)+1),nrow(indgr0))
err<-NULL
errs<-list()
for(i in 1:length(Ks)){
  for(j in 1:nfolds){
    print(paste("fold=",j))
    M_fold<-ppmii[-indgr0[folds[j]:(folds[j+1]-1),][,1],]
    out<-compute_proj0(M=M_fold, K=Ks[i])
    V=out$V
    M_fold<-ppmii[,-indgr0[folds[j]:(folds[j+1]-1),][,2]]
    out<-compute_proj0(M=M_fold, K=Ks[i])
    U=out$U
    Mfold<-ppmii[indgr0[folds[j]:(folds[j+1]-1),][,1],indgr0[folds[j]:(folds[j+1]-1),][,2]]
    err<-c(err,c((ppmii-t(U)%*%V)[indgr0[folds[j]:(folds[j+1]-1),][,1],indgr0[folds[j]:(folds[j+1]-1),][,2]]))
  }
  errs[[i]]<-err
  
}

save(errs, file="errs_cv10_sdgs.RData")
```


```{r}
mae<-rmse<-NULL
for(i in 1:length(errs)){
  mae<-c(mae,mean(abs(errs[[i]])))
  rmse<-c(rmse,sqrt(mean(errs[[i]]^2)))
}

plot(mae)
plot(rmse)#k=4
```























## Choose $\lambda$ and $\tilde k$ by cross validation on corporate reports (CR) data

choose $K$ by topic coherence

```{r}
#compute cooccurrences
myfcm<-fcm(tk,tri=F, context="window", window=10)
fcm<-as.matrix(myfcm)
myfcm2<-fcm(tk22,tri=F, context="window", window=10)
fcm2<-as.matrix(myfcm2)

topwords<-NULL
for(i in 1:nrow(U)){
  topwords<-cbind(topwords,colnames(ppmi)[order(U[i,],decreasing=T)][1:20])
}

#smooth otherwise :0
smooth<-10^(-5)
diag(fcm)<-diag(fcm)+smooth


coh<-text2vec::coherence(topwords,fcm, metrics="mean_npmi",n_doc_tcm = ndoc(mydfm_n))
coh

coh2<-text2vec::coherence(topwords,fcm2, metrics="mean_npmi",n_doc_tcm = ndoc(mydfm_n))
coh2
```

```{r}
lams<-c(0,1,10,20,50)
Ks<-seq(3,15)
cohs<-matrix(NA,length(lams), length(Ks))
outk<-outl<-list()

for(j in 1:length(lams)){
  for(k in 1:length(Ks)){
    out<-compute_proj(M=ppmii, C=ppmii2,K=Ks[k],lam=lams[j])
    U<-out$U
    topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,colnames(ppmi)[order(U[i,],decreasing=T)][1:20])
    }
    coh<-text2vec::coherence(topwords,fcm, metrics="mean_npmi",n_doc_tcm = ndoc(mydfm_n))
    cohs[j,k]<-mean(coh)
    outk[[k]]<-out
  }
  outl[[j]]<-outk
}

save(file="outl.RData",outl)
save(file="cohs.RData",cohs)
```

```{r}
out<-outl[[5]][[6]]
U=out$U
out$props
```

```{r}
lams<-seq(21,30,1)# 0,1,10,20,50 #1 10-20 #2 21-30
Ks<-seq(3,15)
cohs<-matrix(NA,length(lams), length(Ks))
outk<-outl<-list()

for(j in 1:length(lams)){
  for(k in 1:length(Ks)){
    out<-compute_proj(M=ppmii, C=ppmii2,K=Ks[k],lam=lams[j])
    U<-out$U
    topwords<-NULL
    for(i in 1:nrow(U)){
      topwords<-cbind(topwords,colnames(ppmi)[order(U[i,],decreasing=T)][1:20])
    }
    coh<-text2vec::coherence(topwords,fcm, metrics="mean_npmi",n_doc_tcm = ndoc(mydfm_n))
    cohs[j,k]<-mean(coh)
    outk[[k]]<-out
  }
  outl[[j]]<-outk
}

save(file="outl2.RData",outl)
save(file="cohs2.RData",cohs)
lams[5]
```

for the chosen K maximize sum of the explained proportions
```{r}
load(file="outl.RData")
los<-prop<-matrix(NA,length(outl),length(outk))
for(j in 1:length(outl)){
  for(k in 1:length(outk)){
    los[j,k]<-sum(outl[[j]][[k]][[6]])
    prop[j,k]<-outl[[j]][[k]][[5]]
  }
}
los
prop
#lams[5] #25

```

```{r}
out<-outl[[5]][[3]]
U=out$U
out$prop
```

topics
 
```{r}

colnames(ppmi)[order(U[1,],decreasing=T)][1:20]#compliance
colnames(ppmi)[order(U[2,],decreasing=T)][1:20]#ressources dev countries
colnames(ppmi)[order(U[3,],decreasing=T)][1:20]#society
colnames(ppmi)[order(U[4,],decreasing=T)][1:20]#educ skills
colnames(ppmi)[order(U[5,],decreasing=T)][1:20]#product supply chain
#colnames(ppmi)[order(U[6,],decreasing=T)][1:20]#governance
#colnames(ppmi)[order(U[7,],decreasing=T)][1:20]#financial
```
 
 
adjust scores on 17 goals to sum up to one over the topics -> can see the relatedness

```{r}
Q<-out$Q
Qs<-Q/matrix(apply(Q,2,sum),dim(Q)[1],dim(Q)[2],byrow=T)
```



```{r}
nms<-c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
nums<-c(1,2)
plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA)
text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
```


```{r}
nms<-c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
nums<-c(3,5)
plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA)
text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
```


## ALS on pmi

```{r}
update_u_p0<-function(p,M,C,U,V,Q,lam, omit0=T){
  v_p<-V[p,,drop=F]
  q_p<-Q[p,,drop=F]
  W<-W2<-1
  if(omit0){
    W<-matrix(0,dim(M)[1],dim(M)[2])
    W[M>0]<-1
    W2<-matrix(0,dim(C)[1],dim(C)[2])
    W2[C>0]<-1
  }
  sumM<-((M-t(U[-p,])%*%V[-p,])*W)%*%t(v_p)
  sumC<-((C-t(U[-p,])%*%Q[-p,])*W2)%*%t(q_p)
  u_p<-t((sumM+lam*sumC)/(sum((v_p)^2) + lam*sum((q_p)^2)))
  #t(u_p)
  #t(v_p)
  u_p[u_p<0]<-0
  u_p<-u_p/sum(u_p)
  return(u_p)
}

up<-update_u_p0(p=1,M=ppmii,C=ppmii2,U=out$U,V=out$V,Q=out$Q,lam=0.1)
# U<-t(sapply(1:K,update_u_p,M=ppmii,C=ppmii2,U=out$U,V=out$V,Q=out$Q,lam=0.1))
# dim(U)

update_v_p0<-function(p,M,U,V,omit0=T){
  u_p<-U[p,,drop=F]
  W<-1
  if(omit0){
    W<-matrix(0,dim(M)[1],dim(M)[2])
    W[M>0]<-1
 }
  sumM<-u_p%*%((M-t(U[-p,])%*%V[-p,])*W)
  v_p<-((sumM)/(sum((u_p)^2)))
  v_p[v_p<0]<-0
  return(v_p)
}
vp<-update_v_p0(p=1,M=ppmii,U=out$U,V=out$V)
dim(vp)
V<-t(sapply(1:K,update_v_p,M=ppmii,U=out$U,V=out$V))

update_q_p0<-function(p,C,U,Q,omit0=T){
  u_p<-U[p,,drop=F]
  W2<-1
  if(omit0){
    W2<-matrix(0,dim(C)[1],dim(C)[2])
    W2[C>0]<-1
  }
  sumC<-u_p%*%((C-t(U[-p,])%*%Q[-p,])*W2)
  q_p<-t((sumC)/(sum((u_p)^2)))
  q_p[q_p<0]<-0
  #q_p<-q_p/sum(q_p)
  return(q_p)
}

qp<-update_q_p0(p=1,C=ppmii2,U=out$U,Q=out$Q)
dim(qp)
Q<-t(sapply(1:K,update_q_p,C=ppmii2,U=out$U,Q=out$Q))



compute_proj0<-function(M,C,K,lam,thr=0.01,seed=1,iters=100){
  set.seed(seed)
  conv<-F
  i=0
  initMat<-irlba::svdr(ppmii,k=5)
  U<-t(initMat$u)
  U=matrix(runif(dim(M)[1]*K),K,dim(M)[1])
  V=solve((U)%*%t(U))%*%(U)%*%(M); V[V<0]<-0
  Q=solve((U)%*%t(U))%*%(U)%*%(C); Q[Q<0]<-0
  loss<-1000
  print(K)
  while(!conv){
    i<-i+1
    
    
    for(p in 1:K){
      
      U[p,]<-update_u_p0(p=p,M=M,C=C,U=U,V=V,Q=Q,lam=lam)
      V[p,]<-update_v_p0(p=p,M=M,U=U,V=V)
      Q[p,]<-update_q_p0(p=p,C=C,U=U,Q=Q)
    }
    
    
    #check convergence
    loss<-c(loss, sum((M-t(U)%*%V)^2) + lam*sum((C-t(U)%*%Q)^2))
    losses<-c(sum((M-t(U)%*%V)^2),sum((C-t(U)%*%Q)^2))
    crit<-1000
    if(i>3){
      crit<-loss[length(loss)]-loss[length(loss)-1]
    }
    
    if(crit<thr|i>iters){
      conv<-TRUE
      }
    #print(loss[length(loss)])
  }
  #ratio of proportions of the explained variance
  props<-c((1-sum((M-t(U)%*%V)^2)/sum((M)^2)),(1-sum((C-t(U)%*%Q)^2)/sum((C)^2)))
  prop<-props[1]/props[2]
  out<-list(V=V,U=U,Q=Q,lam=lam,prop=prop,props=props,loss=loss,losses=losses,iters=i)
  
  return(out)
}
```


```{r}
out0<-compute_proj0(M=ppmii,C=ppmii2,K=5,lam=20,thr=0.0001,seed=1,iters=100)
```
```{r}
U<-out0$U
colnames(ppmi)[order(U[1,],decreasing=T)][1:20]#compliance
colnames(ppmi)[order(U[2,],decreasing=T)][1:20]#ressources dev countries
colnames(ppmi)[order(U[3,],decreasing=T)][1:20]#society
colnames(ppmi)[order(U[4,],decreasing=T)][1:20]#educ skills
colnames(ppmi)[order(U[5,],decreasing=T)][1:20]#product supply chain
#colnames(ppmi)[order(U[6,],decreasing=T)][1:20]#governance
#colnames(ppmi)[order(U[7,],decreasing=T)][1:20]#financial
```
 
 
adjust scores on 17 goals to sum up to one over the topics -> can see the relatedness

```{r}
Q<-out$Q
Qs<-Q/matrix(apply(Q,2,sum),dim(Q)[1],dim(Q)[2],byrow=T)
```



```{r}
nms<-c(1,2,3,8,9,4,5,10,11,16,17,6,7,12,14,13,15)
nums<-c(2,3)
plot(c(Qs[nums[1],]),c(Qs[nums[2],]),col=NA)
text(c(Qs[nums[1],]),c(Qs[nums[2],]),nms)
```








